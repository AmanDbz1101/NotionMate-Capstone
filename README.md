<div align="center">
  <img src="data/project_logo.jpg" alt="NotionMate Logo" width="200"/>
  
  # NotionMate - AI-Powered Learning Companion

  [![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://www.python.org/)
  [![Streamlit](https://img.shields.io/badge/Streamlit-1.40-red.svg)](https://streamlit.io/)
  [![LangChain](https://img.shields.io/badge/LangChain-0.3-green.svg)](https://langchain.com/)
  [![Google ADK](https://img.shields.io/badge/Google_ADK-1.0-orange.svg)](https://github.com/google/generative-ai-docs)
</div>

## üìã Table of Contents
- [Problem Statement](#-problem-statement)
- [Solution](#-solution)
- [Architecture](#-architecture)
- [Notion Agent Workflow](#-notion-agent-workflow)
- [Project Structure](#-project-structure)
- [Setup Instructions](#-setup-instructions)
- [Usage](#-usage)
- [Technology Stack](#-technology-stack)

---

## üéØ Problem Statement

Modern learners extensively use AI chatbots for education, acquiring knowledge on various topics ranging from technical concepts to life skills. However, a critical gap exists in knowledge retention and organization:

### The Challenge:
- **Manual Summarization**: Users must explicitly ask chatbots to summarize conversations and manually copy content
- **Complex Content Loss**: Formulas, code snippets, and formatted content often break during manual copying
- **Knowledge Fragmentation**: Learned information gets lost in chat histories, requiring tedious searching to retrieve
- **Low Retention**: The friction of manual storage discourages users from archiving valuable knowledge
- **Time-Consuming**: Switching between chat interface, summarization, and note-taking apps disrupts learning flow

This leads to a cycle of **learning, forgetting, and re-searching** instead of **learning, storing, and applying**.

---

## üí° Solution

**NotionMate** bridges the gap between learning and knowledge management with a **one-click solution** to transform chat conversations into organized Notion notes.

### Key Features:
‚úÖ **One-Click Export**: Single button press to save conversation summaries to Notion  
‚úÖ **Smart Categorization**: Choose from predefined Notion pages (Technical Knowledge, Life Lessons, Health Tips, Jokes, etc.)  
‚úÖ **Intelligent Summarization**: AI-powered extraction of key points and main concepts  
‚úÖ **Visual Enhancement**: Automatic relevant image search via Serper API and embedding  
‚úÖ **Format Preservation**: Maintains formulas, code blocks, and complex formatting  
‚úÖ **RAG-Enhanced Chat**: Upload PDFs to create a context-aware knowledge base for conversations  
‚úÖ **Groq-Powered Responses**: Fast and cost-effective API calls using LiteLLM with Groq

### Workflow:
1. Have a conversation with the AI chatbot (with or without uploaded PDFs)
2. Select target Notion page from dropdown menu
3. Click "Write to Notion" button
4. System automatically:
   - Summarizes the entire conversation
   - Extracts key learning points
   - Finds a relevant cover image
   - Formats content with proper Notion blocks
   - Writes to your selected Notion page

---

## üèóÔ∏è Architecture

<div align="center">
  <img src="data/NotionMate Architecture.png" alt="NotionMate Architecture Diagram" width="800"/>
</div>


### Component Breakdown:

#### 1. **Streamlit UI Layer**
- Main entry point for user interaction
- Handles chat interface, PDF uploads, and Notion integration controls
- Uses `ChatGroq` (via LiteLLM) for fast and cost-effective API responses

#### 2. **RAG Chatbot System**
- **Vector Store**: Chroma DB for persistent document embeddings
- **Embeddings**: HuggingFace `sentence-transformers/all-mpnet-base-v2`
- **Memory Creator**: Converts PDFs to searchable vector embeddings
- **LLM**: Groq API (accessed through LiteLLM for improved compatibility)

#### 3. **Notion Agent Orchestrator**
The core innovation of NotionMate - a sequential agent pipeline using **Google ADK (Agent Development Kit)**:

**Agent Pipeline Flow:**
1. **Summary Agent** ‚Üí Analyzes full conversation and extracts key learning points
2. **Topic Agent** ‚Üí Distills summary into a concise 2-4 word topic
3. **Image Search Agent** ‚Üí Finds relevant cover image using Serper API
4. **Formatter Agent** ‚Üí Structures content into Notion block format
5. **Writer Agent** ‚Üí Writes formatted blocks to target Notion page

**All agents use Gemini 2.0 Flash model** for optimal performance and cost efficiency.

**State Management:**
- Uses Google ADK's `InMemorySessionService` for state persistence
- Each agent reads from and writes to shared state dictionary
- Sequential execution ensures data dependencies are met

#### 4. **Notion Integration Layer**
- **Notion MCP Server**: Official `@notionhq/notion-mcp-server` via NPX
- **Communication Protocol**: stdio (standard input/output)
- **Tools Available**:
  - `list_objects`: Retrieve available Notion pages
  - `append_block_children`: Write content blocks to pages
- **Authentication**: Integration token from environment variables

---

## üîÑ Notion Agent Workflow

### Sequential Agent Architecture

The Notion Agent system is built on **Google ADK's Sequential Agent pattern**, where each agent performs a specialized task and passes its output to the next agent in the chain.


<div align="center">
  <img src="data/NotionMate workflow.png" alt="NotionMate Architecture Diagram" width="800"/>
</div>

### Detailed Agent Breakdown:

#### **Agent 1: Summary Agent**
- **Model**: Gemini 2.0 Flash
- **Input**: Complete chat conversation history
- **Task**: Extract and structure key learning points
- **Output**: `summary` (comprehensive markdown summary)
- **Prompt Focus**: "Create a well-structured summary capturing all key points and important details"

#### **Agent 2: Topic Agent**
- **Model**: Gemini 2.0 Flash
- **Input**: `summary` from Agent 1
- **Task**: Extract a concise topic suitable for image search
- **Output**: `topic` (2-4 words)
- **Prompt Focus**: "Extract the main topic that works well as an image search query"

#### **Agent 3: Image Search Agent**
- **Model**: Gemini 2.0 Flash
- **Input**: `topic` from Agent 2
- **Task**: Search for visually relevant image using Serper API
- **Tool**: `search_image_from_state_tool`
  - Reads topic from state
  - Calls Serper API
  - Returns high-quality image URL
- **Output**: `image_url` + `image_search_status`

#### **Agent 4: Formatter Agent**
- **Model**: Gemini 2.0 Flash
- **Input**: `summary`, `topic`, `image_url` from previous agents
- **Task**: Convert content into Notion block format
- **Tool**: `format_notion_blocks_tool`
  - Creates heading block with topic
  - Adds image block with URL
  - Inserts divider
  - Converts summary into bulleted list blocks
- **Output**: `notion_blocks` (JSON array) + `format_status`

**Notion Block Structure:**
```json
[
  {"type": "heading_1", "heading_1": {"rich_text": [{"text": {"content": "Topic"}}]}},
  {"type": "image", "image": {"external": {"url": "image_url"}}},
  {"type": "divider", "divider": {}},
  {"type": "bulleted_list_item", "bulleted_list_item": {"rich_text": [{"text": {"content": "Key point 1"}}]}},
  ...
]
```

#### **Agent 5: Writer Agent**
- **Model**: Gemini 2.0 Flash
- **Input**: `notion_blocks` from Agent 4, `notion_page_id` from user selection
- **Task**: Write blocks to specified Notion page
- **Tool**: `write_to_notion_tool`
  - Connects to Notion MCP Server
  - Calls `append_block_children` with page ID and blocks
  - Returns write confirmation
- **Output**: `write_status` (success/failure) + Notion page link

### State Management Flow:

```python
# Initial State
{
  "chat_history": "User: ... Assistant: ...",
  "notion_page_id": "abc123..."
}

# After Agent 1
{
  ...,
  "summary": "The conversation covered..."
}

# After Agent 2
{
  ...,
  "topic": "Python Decorators"
}

# After Agent 3
{
  ...,
  "image_url": "https://images.unsplash.com/...",
  "image_search_status": "Image found successfully"
}

# After Agent 4
{
  ...,
  "notion_blocks": [{...}, {...}],
  "format_status": "Blocks formatted successfully"
}

# After Agent 5
{
  ...,
  "write_status": "Successfully written to Notion",
  "notion_page_url": "https://notion.so/..."
}
```

### Error Handling:
- Each agent validates its input from previous agents
- If any agent fails, the workflow stops and returns error details
- Notion MCP connection errors are caught and reported to user

---

## üìÅ Project Structure

```
NotionMate-Capstone/
‚îú‚îÄ‚îÄ main.py                          # Main Streamlit application
‚îú‚îÄ‚îÄ notion_mcp_config.py             # Notion MCP server configuration
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îú‚îÄ‚îÄ README.md                        # Project documentation
‚îú‚îÄ‚îÄ .env                             # Environment variables (create this)
‚îú‚îÄ‚îÄ .gitignore                       # Git ignore rules
‚îÇ
‚îú‚îÄ‚îÄ rag_component/                   # RAG chatbot components
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ prompt.py                    # RAG system prompts
‚îÇ   ‚îî‚îÄ‚îÄ memory_creator.py            # Vector store initialization
‚îÇ
‚îú‚îÄ‚îÄ notion_agent/                    # Notion agent orchestration
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                     # Sequential agent workflow coordinator
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ agents/                      # Individual agent definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ summary_agent.py         # Agent 1: Summarizes chat history
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ topic_agent.py           # Agent 2: Extracts topic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image_search_agent.py    # Agent 3: Searches for images
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ formatter_agent.py       # Agent 4: Formats Notion blocks
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ writer_agent.py          # Agent 5: Writes to Notion
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ tools/                       # Agent tools
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ image_search_tool.py     # Serper API integration
‚îÇ       ‚îú‚îÄ‚îÄ notion_formatter_tool.py # Notion block formatter
‚îÇ       ‚îú‚îÄ‚îÄ notion_writer_tool.py    # Notion MCP writer
‚îÇ       ‚îî‚îÄ‚îÄ notion_page_info_retriever.py  # Page list retriever
‚îÇ
‚îú‚îÄ‚îÄ data/                            # User data (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ *.pdf                        # PDF documents for RAG
‚îÇ   ‚îú‚îÄ‚îÄ project_logo.jpg             # Project logo
‚îÇ   ‚îî‚îÄ‚îÄ NotionMate Architecture.png  # Architecture diagram
‚îÇ
‚îú‚îÄ‚îÄ vector_store/                    # Vector database (gitignored)
‚îÇ   ‚îî‚îÄ‚îÄ chroma_index/                # Chroma DB storage
‚îÇ       ‚îî‚îÄ‚îÄ chroma.sqlite3
‚îÇ
‚îî‚îÄ‚îÄ env_notionmate/                  # Virtual environment (gitignored)
```

### Key Files:

- **`main.py`**: Entry point with Streamlit UI, RAG pipeline, and Notion integration
- **`notion_agent/agent.py`**: Orchestrates the 5-agent sequential workflow
- **`notion_mcp_config.py`**: Configures NPX-based Notion MCP server connection
- **`rag_component/memory_creator.py`**: Creates vector embeddings from PDFs

---

## üõ†Ô∏è Setup Instructions

### Prerequisites

1. **Python 3.12+**
2. **Node.js & NPM/NPX** (for Notion MCP Server)
3. **Notion Integration Token** (see below)
4. **Groq API Key** (for LLM responses)
5. **Serper API Key** (for image search)
6. **Google AI API Key** (for Gemini agents)

### Step 1: Clone Repository

```bash
git clone <repository-url>
cd NotionMate-Capstone
```

### Step 2: Create Virtual Environment

```bash
python3.12 -m venv env_notionmate
source env_notionmate/bin/activate  # On Windows: env_notionmate\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**Key Dependencies:**
- `streamlit` - Web UI framework
- `langchain` & `langchain-community` - RAG framework
- `langchain-groq` - Groq LLM integration
- `langchain-google-genai` - Gemini model integration
- `litellm` - Unified LLM API interface (enables Groq support)
- `google-adk` - Google Agent Development Kit for sequential agents
- `chromadb` - Vector database for embeddings
- `langchain-huggingface` - HuggingFace embeddings
- `langchain-mcp-adapters` - Model Context Protocol adapters for Notion

### Step 4: Notion MCP Server Setup

The project uses the official **Notion MCP Server** to interact with Notion's API through the Model Context Protocol.

#### Install NPX (if not already installed):
```bash
npm install -g npx
```

#### Locate NPX Path:
```bash
which npx  # On Linux/Mac
where npx  # On Windows
```
Copy this path for the `.env` file.

#### Configure Notion Integration:
Follow the official Notion MCP setup guide to obtain your integration token:

üìñ **[Notion MCP Server Setup Guide](https://github.com/makenotion/notion-mcp-server)**

You'll need to:
1. Create a Notion integration at https://www.notion.so/my-integrations
2. Grant the integration access to specific pages
3. Copy the integration token

### Step 5: Environment Variables

Create a `.env` file in the project root:

```env
# Notion Configuration
NOTION_TOKEN=your_notion_integration_token_here
NPX_EXECUTABLE_PATH=/usr/local/bin/npx  # Path from 'which npx' command

# LLM API Keys
GROQ_API_KEY=your_groq_api_key_here
GOOGLE_API_KEY=your_google_ai_api_key_here

# Image Search
SERPER_API_KEY=your_serper_api_key_here
```

**Where to get API keys:**
- **Groq API**: https://console.groq.com/keys (Free tier available)
- **Google AI (Gemini)**: https://makersuite.google.com/app/apikey
- **Serper API**: https://serper.dev/api-key

### Step 6: Set Up Vector Store (Optional - for RAG functionality)

If you want to use the PDF upload feature with RAG:

1. Create a `data/` folder in the project root:
```bash
mkdir -p data
```

2. Add PDF files to the `data/` folder

3. Run the memory creator script:
```bash
python rag_component/memory_creator.py
```

This will:
- Load all PDFs from `data/` directory
- Split documents into chunks (500 chars, 50 overlap)
- Generate embeddings using HuggingFace `all-mpnet-base-v2`
- Store in Chroma DB at `vector_store/chroma_index/`
- Persist the vector store for future sessions

**How Memory Creator Works:**
```python
# 1. Load PDFs recursively
loader = DirectoryLoader(data_path, glob="**/*.pdf", loader_cls=PyPDFLoader)
documents = loader.load()

# 2. Split into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
split_docs = text_splitter.split_documents(documents)

# 3. Generate embeddings and store
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
vector_store = Chroma.from_documents(
    split_docs, 
    embedding=embeddings, 
    persist_directory="vector_store/chroma_index"
)
vector_store.persist()
```

The vector store enables context-aware responses by retrieving relevant PDF content during conversations.

### Step 7: Run the Application

```bash
streamlit run main.py
```

The app will open in your default browser at `http://localhost:8501`

---

## üöÄ Usage

### Basic Workflow:

1. **Start a Conversation**
   - Type your questions in the chat interface
   - Optionally upload PDFs for context-aware responses

2. **Select Notion Page**
   - Use the dropdown menu in the sidebar
   - Choose from your available Notion pages (e.g., "Technical Knowledge", "Life Lessons")

3. **Write to Notion**
   - Click the "Write to Notion" button
   - Wait for the agent pipeline to process (typically 10-15 seconds)
   - Receive confirmation with link to your Notion page

### Example Use Cases:

**Technical Learning:**
```
User: Explain Python decorators with examples
[AI provides detailed explanation with code]
‚Üí Select "Technical Knowledge" ‚Üí Write to Notion
Result: Summary with decorator concepts + relevant image
```

**Life Lessons:**
```
User: What are some productivity tips for remote work?
[AI shares various tips and strategies]
‚Üí Select "Life Lessons" ‚Üí Write to Notion
Result: Bullet-pointed productivity tips + cover image
```

**Health & Fitness:**
```
User: Explain the benefits of intermittent fasting
[AI discusses health benefits and mechanisms]
‚Üí Select "Health Knowledge" ‚Üí Write to Notion
Result: Structured health info + relevant imagery
```

---

## üîß Technology Stack

### **Frontend**
- **Streamlit** - Interactive web interface

### **LLM & AI**
- **Groq API** (via LiteLLM) - Fast inference for chatbot responses
- **Google Gemini 2.0 Flash** - All five Notion agents
- **LiteLLM** - Unified API interface for multiple LLM providers

### **RAG System**
- **LangChain** - RAG orchestration framework
- **Chroma DB** - Vector database for embeddings
- **HuggingFace Transformers** - `sentence-transformers/all-mpnet-base-v2` embeddings

### **Agent Framework**
- **Google ADK** - Agent Development Kit for sequential workflows
- **InMemorySessionService** - State management across agents

### **Notion Integration**
- **Notion MCP Server** - Official `@notionhq/notion-mcp-server` via NPX
- **langchain-mcp-adapters** - MCP protocol adapters

### **External APIs**
- **Serper API** - Image search and retrieval
- **Notion API** - Page creation and content management

---

## üöÄ Deployment

### Streamlit Cloud Deployment

This application is deployed on **Streamlit Cloud** and is publicly accessible.

#### Deployment Configuration:

1. **packages.txt**: Added to enable NPX support in Streamlit Cloud environment
   ```
   nodejs
   npm
   ```

2. **Streamlit Secrets Configuration**:
   
   In Streamlit Cloud, secrets are configured differently than local `.env` files. The following environment variables were set:

   ```toml
   # In Streamlit Cloud Secrets (Settings ‚Üí Secrets)
   HUGGINGFACE_API_KEY = "your_key_here"
   GOOGLE_API_KEY = "your_key_here"
   GROQ_API_KEY = "your_key_here"
   SERPER_API_KEY = "your_key_here"
   NOTION_TOKEN = "your_token_here"
   
   # For Streamlit Cloud deployment, use simplified NPX path
   NPX_EXECUTABLE_PATH = "npx"
   ```

   **Note**: On Streamlit Cloud, `NPX_EXECUTABLE_PATH` is set to simply `"npx"` instead of the full path, as Node.js and NPX are installed globally via `packages.txt`.

3. **Deployment Steps**:
   - Push code to GitHub repository
   - Connect repository to Streamlit Cloud
   - Add `packages.txt` with Node.js dependencies
   - Configure secrets in Streamlit Cloud dashboard
   - Deploy!

4. **Key Differences from Local Setup**:
   - Local: Uses full NPX path (e.g., `/home/user/.nvm/versions/node/v24.11.1/bin/npx`)
   - Streamlit Cloud: Uses `"npx"` directly (installed via `packages.txt`)

---

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## üìù License

This project is open source and available under the MIT License.

---

## üôè Acknowledgments

- [Notion MCP Server](https://github.com/makenotion/notion-mcp-server) - Official Notion integration
- [Google ADK](https://github.com/google/generative-ai-docs) - Agent framework
- [LangChain](https://langchain.com/) - RAG framework
- [Groq](https://groq.com/) - Fast LLM inference
- [Serper](https://serper.dev/) - Image search API

---

